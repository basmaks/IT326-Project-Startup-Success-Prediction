---
title: "Startup Success Prediction Model"
output: html_notebook
---
<br>
Introduction to Data Mining (IT326) Project -- Phase 3 <br>
**Prepared by:** Basma Alsulaim and Aafia Nawal Muhammad <br><br>

## 1- Problem 

&nbsp;&nbsp;&nbsp; The problem we aim to address revolves around predicting the success of startups based on historical data encompassing their early-stage decisions. The dataset provides a comprehensive record of startup companies, tracing their early-stage decisions, such as funding rounds, categories, and more, spanning the years from 1984 to 2010. We strive to build a predictive model, particularly using Decision Trees, to anticipate the outcome of startups. 

&nbsp;&nbsp;&nbsp; The ability to predict startup success is important for both investors and job seekers. For investors, it serves as a strategic tool to identify startups with a higher likelihood of success, leading to more informed investment decisions and better returns. On the flip side, job seekers can benefit by identifying promising companies, increasing the likelihood of a fruitful and stable career path.<br><br><br>

## 2- Data Mining Task

&nbsp;&nbsp;&nbsp; The data mining task involves two key aspects: classification and clustering. In the classification part, the goal is to create a predictive model that sorts startups into 'acquired' or 'closed' categories using the class label 'status'. On the clustering part, the aim is to uncover patterns and structures within the dataset, identifying groups of startups with similar traits. This dual approach aims to provide a comprehensive understanding of startup outcomes, offering predictive insights and revealing underlying structures in the startup dataset.

#### Goals:

1. **Prediction:** Create a predictive model to predict if a startup will succeed or close, using past data for better decision-making.  

2. **Pattern Discovery:** Find hidden patterns in startup data, helping us understand common traits among them.

3. **Smart Investing:** Assist investors in making strategic decisions by identifying startups with a high chance of success, leading to better returns.<br><br><br>


## 3- Data Description

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**◆ Source:** Access the dataset on Kaggle through the following link: https://www.kaggle.com/datasets/manishkc06/startup-success-prediction   

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**◆ Number of objects:** The dataset holds 925 rows of data before preprocessing.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**◆ Number of attributes:** The dataset holds 50 columns of data before preprocessing.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**◆ Class label:** The class label for this data set is **status**, which holdes two states: "acquired" (i.e. successful company) or "closed" (i.e. unsuccessful company).

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**◆ Types of attributes:** Nominal, Numerical, Binary<br><br>

### Data Disctionary: 

| Attribute Name           | Description                                                  | Type      | Possible Values                                 |
| ------------------------ | ------------------------------------------------------------ | --------- | ----------------------------------------------- |
| Unnamed: 0               | Method of numbering companies.                               | Nominal   | 1 to 1153                                       |
| state_code               | The code of the state the startup was founded in.            | Nominal   | Different state codes. Like CA, AZ              |
| latitude                 | The latitude of the startup headquarters.                    | Numerical | 90 to -90                                       |
| longitude                | The longitude of the startup headquarters.                   | Numerical | 0 to 180                                        |
| zip_code                 | The zip code of the city the startup was founded in.         | Nominal   | Different random city zipcodes                  |
| id                       | Irrelevant\*                                                 | Nominal   | Data in unrecognizable format c:number          |
| city                     | The name of the city the startup was founded in.             | Nominal   | Different cities. Like Palo Alto, Mountain View |
| Unnamed: 6               | The address of the headquarters.                             | Nominal   | Different addresses. Like San Diego CA 92121    |
| name                     | The name of the startup.                                     | Nominal   | Different names. Like Qsecure, drop.io          |
| labels                   | Irrelevant\*                                                 | Binary    | 0 or 1                                          |
| founded_at               | Startup founding date.                                       | Nominal   | 01 1984 to 09 2010                              |
| closed_at                | Startup closing date.                                        | Nominal   | 01 2001 to 08 2013                              |
| first_funding_at         | Startup first funding date.                                  | Nominal   | 01 2000 to 09 2009                              |
| last_funding_at          | Startup last funding date.                                   | Nominal   | 01 2001 to 09 2011                              |
| age_first_funding_year   | The average age of startup when received first funding.      | Numerical | 0 to 21                                         |
| age_last_funding_year    | The average age of startup when received last funding.       | Numerical | 0 to 21                                         |
| age_first_milestone_year | The average age of startup when acheived first milestone.    | Numerical | 0 to 24                                         |
| age_last_milestone_year  | The average age of startup when acheived last milestone.     | Numerical | 0 to 24                                         |
| relationships            | Irrelevant\*                                                 | Numerical | 0 to 63                                         |
| funding_rounds           | The number of funding rounds the startup went through.       | Numerical | 1 to 10                                         |
| funding_total_usd        | The total number of money in USD raised by the startup.      | Numerical | 11000 to 5700000000                             |
| milestones               | The total number of milestones achieved by the startup.      | Numerical | 0 to 8                                          |
| state_code.1             | The code of the state the startup was founded in.            | Nominal   | Different state codes. Like CA, AZ              |
| is_CA                    | If the startup was founded in California.                    | Binary    | 0 or 1                                          |
| is_NY                    | If the startup was founded in New York.                      | Binary    | 0 or 1                                          |
| is_MA                    | If the startup was founded in Massachusetts.                 | Binary    | 0 or 1                                          |
| is_TX                    | If the startup was founded in Texas.                         | Binary    | 0 or 1                                          |
| is_otherstate            | If the startup was founded in a state other than the listed. | Binary    | 0 or 1                                          |
| category_code            | The sector of the startup.                                   | Nominal   | Different categories. Like biotech, mobile      |
| is_software              | If the startup sector is software.                           | Binary    | 0 or 1                                          |
| is_web                   | If the startup sector is web.                                | Binary    | 0 or 1                                          |
| is_mobile                | If the startup sector is mobile.                             | Binary    | 0 or 1                                          |
| is_enterprise            | If the startup sector is enterprise.                         | Binary    | 0 or 1                                          |
| is_advertising           | If the startup sector is advertising.                        | Binary    | 0 or 1                                          |
| is_gamesvideo            | If the startup sector is video games.                        | Binary    | 0 or 1                                          |
| is_ecommerce             | If the startup sector is ecommerce.                          | Binary    | 0 or 1                                          |
| is_biotech               | If the startup sector is biotech.                            | Binary    | 0 or 1                                          |
| is_consulting            | If the startup sector is consulting.                         | Binary    | 0 or 1                                          |
| is_othercategory         | If the startup sector is any other category that the listed. | Binary    | 0 or 1                                          |
| object_id                | Irrelevant\*                                                 | Nominal   | Data in unrecognizable format c:number          |
| has_VC                   | If the startup has a venture capitalist\*\* investor.        | Binary    | 0 or 1                                          |
| has_angel                | If the startup has an angel\*\*\* investor.                  | Binary    | 0 or 1                                          |
| has_roundA               | If the startup went through a series A funding round.        | Binary    | 0 or 1                                          |
| has_roundB               | If the startup went through a series B funding round.        | Binary    | 0 or 1                                          |
| has_roundC               | If the startup went through a series C funding round.        | Binary    | 0 or 1                                          |
| has_roundD               | If the startup went through a series D funding round.        | Binary    | 0 or 1                                          |
| avg_participants         | The average number of participants in the startup.           | Binary    | 0 or 1                                          |
| is_top500                | If the startup is a top 500 company.                         | Binary    | 0 or 1                                          |
| status                   | The acquisition status of the startup.                       | Binary    | "acquired" or "closed"                          |

\*The original dataset source lacks any description for these attributes. We can only speculate their meanings. They could refer to the number of data objects, a method for organizing data, or a recording system of some sort. Furthermore, since they pose no importance to the startup prediction model, we will designate them as **"irrelevant"** and proceed to remove them from the dataset in the subsequent steps. 

\*\***VC** stands for Venture Capitalist. Which is a type of investor that invests the money of a venture capital firm into small startup companies.

\*\*\***Angel** stands for Angel Investor. Which is a type of investor that invests their personal money  into small startup companies in exchange for a percentage in the company.<br>

<br>
### Installing necessary packages:
```{r}
install.packages("readxl")
install.packages("lubridate")
install.packages("scatterplot3d") 
install.packages("dplyr")
install.packages("ggplot2")
install.packages("colorspace")
```
<br>
#### Import Dataset
```{r}
library(readxl)
dataset <- read_excel("Original_StartupData.xlsx")
View(dataset)
```
To import and load the excel dataset for data preprocessing and mining. <br><br>


### Assessing Class Label Balance Before Pre-Processing 

```{r}
library(ggplot2)
library(grid)

# Create a bar plot for the "status" attribute
gg <- ggplot(dataset, aes(x = status)) +
  geom_bar(fill = "darkgray", color = "black") +
  labs(title = "Distribution of Startup Status", x = "Status", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))

# Print the plot
print(gg)

# Add an external annotation to the right side
grid.text("1 = Acquired\n0 = Closed", x = 0.9, y = 0.92, just = c("right", "top"), gp = gpar(fontsize = 12, col = "black"))
```

As shown above, **there is a class imbalance in the class label (status)**. In the dataset, objects with acquired status are almost twice as much as objects with closed objects. This indicates that the class "acquired" is more prevalent than the class "closed" in the dataset. In later steps, this may present a challenge in training and testing the model. <br><br>

### Sample of Raw Dataset and Summary:

Here, we aim to explore the data of our dataset. Five-number summary, variance, missing values, and graphs are utilized to give insight into the data we are dealing with. Nominal, numerical, and binary data will be examined.<br>

```{r}
str(dataset)
```
This is what the raw dataset looks like: a collection of different data types. <br><br>

#### A) Numerical attributes:

##### i) Five-number summary:

Below, a five-number summary is performed on numerical attributes to gain insights into the distribution and characteristics of the numeric data. The five number summary is represented by the minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum of the available numeric attributes. 

```{r}
# Specify numerical attributes for the five-number summary
numerical_attributes <- c("latitude", "longitude", 
                          "age_first_funding_year", "age_last_funding_year", 
                          "age_first_milestone_year", "age_last_milestone_year", 
                          "relationships", "funding_rounds", "funding_total_usd", 
                          "milestones", "avg_participants")

# Subset the data to include only numerical attributes
numerical_data <- dataset[, numerical_attributes]

# Calculate the five-number summary
summary_data <- summary(numerical_data)
print(summary_data)
```
As demonstrated above by the five-number summary, some attributes contain negative values. Negative values in attributes related to age and longitude are illogical and likely errors. We will address this issue by removing these negative values during the data cleaning process. <br>

##### ii) Variance:

```{r}
# List of numerical attributes
numerical_attributes <- c("latitude", "longitude",
                          "age_first_funding_year", "age_last_funding_year",
                          "age_first_milestone_year", "age_last_milestone_year",
                          "relationships", "funding_rounds", "funding_total_usd",
                          "milestones", "avg_participants")

# Calculate variances for numerical attributes
variances_numerical <- sapply(dataset[, numerical_attributes], var)

# Print variances for numerical attributes
for (i in seq_along(numerical_attributes)) {
  cat("Variance for", paste(numerical_attributes[i], ":", variances_numerical[i]), "\n")
}
```
**The variance can be classified into four classes: low, moderate, high, and very high.** 
Attributes age_first_milestone_year and age_last_milestone_year include missing values which prevents an accurate assessment of variability. 

Attributes longitude, and funding_total_usd attribute demonstrates an **exceptionally high level of variability**, likely influenced by outliers or a broad range of funding values.

The relationships attribute shows a **high level of variability**, indicating a wide range of values among startups.

Attributes latitude and age_last_funding_year display a **moderate level of variance** with respect to the dataset. 

Attributes of **low variance** are age_first_funding_year, funding_rounds, and milestones, which might be due to their minimal range resulting in a more concentrated distribution of values. Which brings us to the third numerical data assessment metric: <br>


##### iii) Mising values:
```{r}
# List of numerical attributes
numerical_attributes <- c("latitude", "longitude", "zip_code", 
                          "age_first_funding_year", "age_last_funding_year", 
                          "age_first_milestone_year", "age_last_milestone_year", 
                          "relationships", "funding_rounds", "funding_total_usd", 
                          "milestones", "avg_participants")

missing_numerical_values <- sapply(dataset[, numerical_attributes], function(x) sum(is.na(x)))
total_numerical_values <- sum(missing_numerical_values)

print(total_numerical_values)
```
There is a total of 304 missing values in the columns of numerical attributes. We'll address this issue by removing these missing values during the data cleaning process. <br><br>

#### B) Binary and Nominal Attributes:

##### i) Five-number summary:

Since we are addressing binary and nominal data, the five-number summary won't give us useful insights. Therefore, we won't do the five-number summary for those types of attributes. <br>

##### ii) Variance:

Variance cannot be performed on nominal attributes, so we will only conduct it on binary attributes. 

```{r}
# List of binary attributes
binary_attributes <- c("is_CA", "is_NY", "is_MA", "is_TX", "is_otherstate",
                        "is_software", "is_web", "is_mobile", "is_enterprise",
                        "is_advertising", "is_gamesvideo", "is_ecommerce",
                        "is_biotech", "is_consulting", "is_othercategory",
                        "has_VC", "has_angel", "has_roundA", "has_roundB",
                        "has_roundC", "has_roundD", "labels", "is_top500", "status")

# Calculate variances for binary attributes
variances_binary <- sapply(dataset[, binary_attributes], var)

# Print variances for binary attributes
for (i in seq_along(binary_attributes)) {
  cat("Variance for", paste(binary_attributes[i], ":", variances_binary[i]), "\n")
}
```
These values indicate how much the values in each binary attribute deviate from their mean (0.5 for balanced binary variables). Lower variances suggest that the values are closer to the mean, while higher variances indicate greater spread. Since the variance for all these binary attributes is very low, the variance indicates no further significance. 

Although status is a binary attribute, it hasn't been encoded to 1 (acquired) and 0 (closed) yet. So, its variance is denoted by NA. As it seems a nominal attribute. <br>

##### iii) Mising values:

```{r}
# List of attributes
binary_nominal_attributes <- c("Unnamed: 0", "state_code", "id", "zip_code", "city", "Unnamed: 6",
                          "name", "founded_at", "closed_at", "first_funding_at",
                          "last_funding_at", "state_code.1", "category_code",
                          "object_id", "is_CA", "is_NY", "is_MA", "is_TX", "is_otherstate",
                          "is_software", "is_web", "is_mobile", "is_enterprise",
                          "is_advertising", "is_gamesvideo", "is_ecommerce",
                          "is_biotech", "is_consulting", "is_othercategory",
                          "has_VC", "has_angel", "has_roundA", "has_roundB",
                          "has_roundC", "has_roundD", "labels", "is_top500", "status")

missing_binary_nominal_values <- sapply(dataset[, binary_nominal_attributes], function(x) sum(is.na(x)))
total_binary_nominal_values <- sum(missing_binary_nominal_values)

print(total_binary_nominal_values)
```
There is a total of 1082 missing values in the columns of binary and nominal attributes. We'll address this issue by removing these missing values during the data cleaning process. <br>


##### Detecting and Dealing With Missing Values 

We will detect and eliminate missing values in the dataset to ensure the creation of representative graphs, as plotting requires addressing missing values first.

```{r}
sum(is.na(dataset))
```
There is a total of 1386 missing values in the startup dataset. 

```{r}
# Calculate the sum of missing values for each attribute
sum_missing_values <- function(attribute) {
  sum(is.na(attribute))
}

# Apply the function to each attribute and store the results in a data frame
missing_values <- data.frame(
  Attribute = names(dataset),
  Missing_Values = sapply(dataset, sum_missing_values)
)

# Print the result
print(missing_values)
```

The table above shows each attribute with the number of missing values found in its column. All missing values come from five attributes, ordered from most missing to least: closed_at, Unnamed: 6, age_first_milestone_year, 	age_last_milestone_year, and 	state_code.1. 

#### Replacing Missing Values

For attribute closed_at and state_code.1, we will fill all missing values with the global constant N/A because information about the operation of the company (whether is is still open or closed) remains unknown and cannot be replaced with the average as the company may still be operating. 

Note: state_code.1 is a duplicate attribute. Unnamed: 6 is an irrelevant attribute as explained at the beginning of this section. Both state_code.1 and Unnamed: 6 will be removed in the data reduction step. 

#### Before replacing missing values of attribute closed_at with N/A:
```{r}
head(dataset$closed_at)
```
As shown in the table above, missing values are automatically denoted as "NA" in R. 

#### Replacing with "N/A":
```{r}
dataset$closed_at[is.na(dataset$closed_at)] <- "N/A"
dataset$state_code.1[is.na(dataset$state_code.1)] <- "N/A"
dataset$'Unnamed: 6'[is.na(dataset$'Unnamed: 6')] <- "N/A"
```
This code chunk replaces all missing values in attribute closed_at with N/A.

#### After replacing missing values of attribute closed_at with N/A:
```{r}
head(dataset$closed_at)
```
As shown in the table above, missing values are replaced with global constant "N/A". 


For attributes age_first_milestone_year and age_last_milestone_year we will replace missing values with the attribute's mean. <br>

#### Before replacing missing values of attributes age_first_milestone_year and age_last_milestone_year with average:
```{r}
row_13 <- dataset[13, c("age_first_milestone_year", "age_last_milestone_year")]

# Display the result
print(row_13)
```
As shown in row 13 of the dataset, missing values are automatically denoted as "NA" in R. 

#### Replacing with average:
```{r}
dataset$age_first_milestone_year = ifelse (is.na(dataset$age_first_milestone_year), ave(dataset$age_first_milestone_year, FUN=function(x)mean(x,na.rm=TRUE)), dataset$age_first_milestone_year)

dataset$age_last_milestone_year = ifelse (is.na(dataset$age_last_milestone_year), ave(dataset$age_last_milestone_year, FUN=function(x)mean(x,na.rm=TRUE)), dataset$age_last_milestone_year)
```
This code chunk replaces all missing values of attributes age_first_milestone_year and age_last_milestone_year with their average.

#### After replacing missing values of attributes age_first_milestone_year and age_last_milestone_year with average:
```{r}
row_13 <- dataset[13, c("age_first_milestone_year", "age_last_milestone_year")]

# Display the result
print(row_13)
```
As shown in row 13 of the dataset, missing values are replaced with the attribute average. 

#### Check for remaining missing values:
```{r}
sum(is.na(dataset))
```
All missing values are addressed. There are no remaining missing values. 


### Graphical Representation of the Dataset ( Before ) Data Pre-Processing 
<br>

#### A) Numerical Attributes Histogram:
```{r}

```

```{r}
# Load the required libraries
library(ggplot2)

# Select numerical attributes for histograms
numerical_attributes <- c("latitude", "longitude",
                           "age_first_funding_year", "age_last_funding_year",
                           "age_first_milestone_year", "age_last_milestone_year",
                           "relationships", "funding_rounds", "funding_total_usd",
                           "milestones", "avg_participants")

# Melt the dataset for easier plotting
melted_data <- reshape2::melt(dataset[, numerical_attributes])

# Create histogram with facet wrap
histogram_plot_numerical_before <- ggplot(melted_data, aes(x = value)) +
  geom_histogram(binwidth = 1, fill = "darkgray", color = "black") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Histogram for Numerical Attributes BEFORE Pre-processing", x = "Value", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5))

# Display the plot
par(mfrow = c(1, 2))  # Set up a 1x2 plotting grid
plot(histogram_plot_numerical_before)  # Plot the original graph
```
In the collection of histograms above, we can see that there is a noticeable distribution among values of certain attributes. As shown, "funding_total_usd" is not appearing as expected, it could be due to the presence of extreme values or outliers that are affecting the visualization. The graphs above indicate that we must remove outliers and deal with negative values. 

```{r}
# Assuming your dataset is named 'your_dataset'
# Replace 'your_dataset' with the actual name of your dataset

# Select numerical attributes for histograms
numerical_attributes <- c("latitude", "longitude",
                           "age_first_funding_year", "age_last_funding_year",
                           "age_first_milestone_year", "age_last_milestone_year",
                           "relationships", "funding_rounds", "funding_total_usd",
                           "milestones", "avg_participants")

# Melt the dataset for easier plotting
melted_data <- reshape2::melt(dataset[, numerical_attributes])

# Create histogram with facet wrap, log-transforming funding_total_usd
histogram_plot <- ggplot(melted_data, aes(x = log(value + 1))) +
  geom_histogram(binwidth = 0.2, fill = "darkgray", color = "black") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Histogram for Numerical Attributes BEFORE Pre-processing",
       x = "Log(Value + 1)",
       y = "Frequency")+
  theme(plot.title = element_text(hjust = 0.5))

# Display the plot
print(histogram_plot)
```
Here, we fixed the issue regarding the "funding_total_usd" graph. We used the log(value + 1) to log-transform the "funding_total_usd" values. Although the "funding_total_usd" graph appeared, we have eliminated negative values. This indicates that the graphical representation above is not an accurate representation of the raw numerical attributes but an estimate of one. <br>

#### B) Nominal Attributes Barplot:

Attributes Unnamed: 0, id, zip_code, Unnamed: 6, object_id are used for identification purposes and do not have mathematical significance. And since they are unique unrepetitive strings of numbers, a barplot is not necessary to represent them. In addition, they will be removed later in the data reduction step. 

Moreover, attributes founded_at,	closed_at,	first_funding_at, and	last_funding_at represent dates in the format mm/yy/dd. We can say that they are semi-unique since the probability of two rows sharing the same date is extremely low and holds no significance. Therefore, no barplot is necessary to represent them.  

```{r}
library(tidyr)

# Selecting specific columns for visualization
columns_to_visualize <- dataset[, c("state_code", "city", "category_code", "state_code.1")]

# Melt the required columns for visualization
melted_data_before <- gather(data = columns_to_visualize)

# Plotting bar graphs for the specified attributes and facet_wrap
barplot_nominal_before <- ggplot(melted_data_before, aes(x = value, fill = key)) +
  geom_bar(position = "dodge", stat = "count", color = "black", fill = "darkgray") +
  facet_wrap(~key, scales = "free") +
  labs(title = "Nominal Attributes BEFORE Pre-processing", x = "Values", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))

# Displaying the plot
print(barplot_nominal_before)
```
From the graphs, we can see that state_code and state_code.1 are redundant which requires the elimination of one of them later in the data reduction step. In addition, we can see which state code, city, and category code most startups shared. 

Attributes Unnamed: 0, id, zip_code, Unnamed: 6, and object_id cannot be plotted as they are unique attributes. They pertain no significance to the dataset and will be removed later on in data reduction. <br> 

#### C) Binary Attributes Barplot:

```{r}
library(tidyr)

# List of binary attributes
binary_attributes <- c("is_CA", "is_NY", "is_MA", "is_TX", "is_otherstate", 
                       "is_software", "is_web", "is_mobile", "is_enterprise", 
                       "is_advertising", "is_gamesvideo", "is_ecommerce", 
                       "is_biotech", "is_consulting", "is_othercategory", 
                       "has_VC", "has_angel", "has_roundA", "has_roundB", 
                       "has_roundC", "has_roundD", "labels", "is_top500", "status")

# Melt the datasets for easier plotting
melted_data <- gather(dataset, key = "variable", value = "value", all_of(binary_attributes))

# Create bar plots with facet wrap
barplot_binary_before <- ggplot(melted_data, aes(x = value, fill = variable)) +
  geom_bar(position = "dodge", stat = "count", color = "black", fill = "darkgray") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Bar Plots for Binary Attributes BEFORE Pre-processing", x = "Value", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))

# Display the plot
print(barplot_binary_before)
```
The barplots provide insights into the startup landscape from 1984 to 2010, revealing trends such as the states with the highest startup activity, the most prevalent category types, preferred investment rounds, the likelihood of a startup becoming a top 500 company, and their overall status.<br><br><br>


## 4-Data Pre-processing 

Data pre-procesing is essential in producing accurate results. Because correct pre-processing leads to correct results, and vice versa. We will prepare the startup data for data mining by following the data pre-processing steps: 
1-Data Cleaning 
2-Data Integration
3-Data Reduction
4-Data Transformation
<br><br>

## 4.1-Data Cleaning 

In data cleaning we are ensuring accuracy and reliability by identifying replacing negative values and removing outliers from the dataset. <br>

### A) Negative Values

Negative values in attributes related to age and longitude do not make sense. So, we are going to remove the negative sign from the values. 

##### Before removing negative sign of attributes age_first_funding_year, age_last_funding_year, age_first_milestone_year, age_last_milestone_year, and longitude:
```{r}
row_235 <- dataset[235, c("longitude", "age_first_funding_year", "age_last_funding_year", "age_first_milestone_year", "age_last_milestone_year")]

# Display the result
print(row_235)
```
As depicted, attributes above contain negative values. 

##### Removing negative values:
```{r}
# Remove the negative sign from age_first_funding_year
dataset$age_first_funding_year <- abs(dataset$age_first_funding_year)

# Remove the negative sign from age_last_funding_year
dataset$age_last_funding_year <- abs(dataset$age_last_funding_year)

# Remove the negative sign from age_first_milestone_year
dataset$age_first_milestone_year <- abs(dataset$age_first_milestone_year)

# Remove the negative sign from age_last_milestone_year
dataset$age_last_milestone_year <- abs(dataset$age_last_milestone_year)

# Remove the negative sign from longitude
dataset$longitude <- abs(dataset$longitude)
```
This code chunk removes the negative sign from the attributes. 

##### After removing negative sign of attributes age_first_funding_year, age_last_funding_year, age_first_milestone_year, age_last_milestone_year, and longitude:
```{r}
row_235 <- dataset[235, c("longitude", "age_first_funding_year", "age_last_funding_year", "age_first_milestone_year", "age_last_milestone_year")]

# Display the result
print(row_235)
```
As shown, none of the attributes above have a negative sign as they have all become positive numbers. <br>


### B) Outliers 

We will identify and eliminate outliers present in the numerical attributes of the dataset. Outliers, or data points that significantly deviate from the majority, can skew statistical analyses and affect the accuracy of models. By detecting and removing these outliers, we aim to ensure a more robust and representative dataset for subsequent analyses.

Binary and nominal attributes, are discrete and categorical. They don't exhibit outliers as numerical values do. Outliers are specific to numerical data where values significantly deviate from the rest of the dataset. In the case of binary attributes, these represent two categories (0 or 1), so there aren't outliers as there's no numerical range or sequence to measure extremes. For nominal attributes, which represent categories without inherent order (like names) outliers don't exist in the same sense as they do in numerical data. Outliers in numerical data might suggest errors, anomalies, or extremes in the data. 

##### Numerical Attribute Boxplots Before Removing Outliers:

```{r}
library(ggplot2)

# Select numerical attributes for boxplots
numerical_attributes <- c("latitude", "longitude",
                           "age_first_funding_year", "age_last_funding_year",
                           "age_first_milestone_year", "age_last_milestone_year",
                           "relationships", "funding_rounds", "funding_total_usd",
                           "milestones", "avg_participants")

# Melt the dataset for easier plotting
melted_data <- reshape2::melt(dataset[, numerical_attributes])

# Create boxplots with facet wrap
boxplot_plot <- ggplot(melted_data, aes(x = variable, y = value)) +
  geom_boxplot(fill = "darkgray", color = "black") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Boxplots for Numerical Attributes Before Removing Outliers",
       x = "Attribute",
       y = "Value") +
  theme(plot.title = element_text(hjust = 0.5))

# Display the plot
print(boxplot_plot)

```
The box represents the interquartile range (IQR), which is the range between the first quartile (Q1) and the third quartile (Q3). The length of the box indicates the spread of the middle 50% of the data. The whiskers extend from the box to the minimum and maximum values within a certain range. By default, this range is 1.5 times the IQR. Points beyond the whiskers are outliers. As demonstrated above, every boxplot contains outliers that fall beyond the whiskers represented by individual points. 

##### Overview of outliers: 

```{r}
# Attributes to check for outliers
attributes_to_check <- c("latitude", "longitude", "age_first_funding_year", 
                         "age_last_funding_year", "age_first_milestone_year", 
                         "age_last_milestone_year", "relationships", "funding_rounds", 
                         "funding_total_usd", "milestones", "avg_participants")

# Calculate the number of outliers for each attribute
outlier_counts <- sapply(attributes_to_check, function(attr) {
  Q1 <- quantile(dataset[[attr]], 0.25, na.rm = TRUE)
  Q3 <- quantile(dataset[[attr]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  sum(dataset[[attr]] < lower_bound | dataset[[attr]] > upper_bound, na.rm = TRUE)
})

# Create a table with attribute names and their respective outlier counts
outlier_table <- data.frame(Attribute = attributes_to_check, Outlier_Count = outlier_counts)
print(outlier_table)
```
The table above represents the number of outliers for each attribute. 


##### Removing Outliers: 

```{r}
## age_first_funding_year

# Calculate the quartiles
Q1 <- quantile(dataset$age_first_funding_year, 0.25)
Q3 <- quantile(dataset$age_first_funding_year, 0.75)

# Calculate the IQR
IQR <- Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

# Remove outliers from the dataset using dplyr
library(dplyr)

dataset_clean1 <- dataset %>%
  filter(age_first_funding_year >= lower_bound, age_first_funding_year <= upper_bound)

# Print the dimensions of the cleaned dataset
cat("funding_total_usd","\n")
cat("Original dataset dimensions: ", dim(dataset), "\n")
cat("Cleaned dataset dimensions: ", dim(dataset_clean1), "\n","\n")
dataset <- dataset_clean1

#################################
## age_last_funding_year

Q1 <- quantile(dataset$age_last_funding_year, 0.25)
Q3 <- quantile(dataset$age_last_funding_year, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
library(dplyr)
dataset_clean2 <- dataset %>%
  filter(age_last_funding_year >= lower_bound, age_last_funding_year <= upper_bound)

cat("age_last_funding_year","\n")
cat("Original dataset dimensions: ", dim(dataset), "\n")
cat("Cleaned dataset dimensions: ", dim(dataset_clean2), "\n","\n")
dataset <- dataset_clean2

#################################
## age_first_milestone_year

Q1 <- quantile(dataset$age_first_milestone_year, 0.25)
Q3 <- quantile(dataset$age_first_milestone_year, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
library(dplyr)
dataset_clean3 <- dataset %>%
  filter(age_first_milestone_year >= lower_bound, age_first_milestone_year <= upper_bound)

cat("age_first_milestone_year","\n")
cat("Original dataset dimensions: ", dim(dataset), "\n")
cat("Cleaned dataset dimensions: ", dim(dataset_clean3), "\n","\n")
dataset <- dataset_clean3

#################################
## age_last_milestone_year

Q1 <- quantile(dataset$age_last_milestone_year, 0.25)
Q3 <- quantile(dataset$age_last_milestone_year, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
library(dplyr)
dataset_clean4 <- dataset %>%
  filter(age_last_milestone_year >= lower_bound, age_last_milestone_year <= upper_bound)

cat("age_last_milestone_year","\n")
cat("Original dataset dimensions: ", dim(dataset), "\n")
cat("Cleaned dataset dimensions: ", dim(dataset_clean4), "\n","\n")
dataset <- dataset_clean4

#################################
## relationships

Q1 <- quantile(dataset$relationships, 0.25)
Q3 <- quantile(dataset$relationships, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
library(dplyr)
dataset_clean5 <- dataset %>%
  filter(relationships >= lower_bound, relationships <= upper_bound)

cat("relationships","\n")
cat("Original dataset dimensions: ", dim(dataset), "\n")
cat("Cleaned dataset dimensions: ", dim(dataset_clean5), "\n","\n")
dataset <- dataset_clean5

#################################
## funding_rounds

Q1 <- quantile(dataset$funding_rounds, 0.25)
Q3 <- quantile(dataset$funding_rounds, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
library(dplyr)
dataset_clean6 <- dataset %>%
  filter(funding_rounds >= lower_bound, funding_rounds <= upper_bound)

cat("funding_rounds","\n")
cat("Original dataset dimensions: ", dim(dataset), "\n")
cat("Cleaned dataset dimensions: ", dim(dataset_clean6), "\n","\n")
dataset <- dataset_clean6

#################################
## funding_total_usd

Q1 <- quantile(dataset$funding_total_usd, 0.25)
Q3 <- quantile(dataset$funding_total_usd, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
library(dplyr)
dataset_clean7 <- dataset %>%
  filter(funding_total_usd >= lower_bound, funding_total_usd <= upper_bound)

cat("funding_total_usd","\n")
cat("Original dataset dimensions: ", dim(dataset), "\n")
cat("Cleaned dataset dimensions: ", dim(dataset_clean7), "\n","\n")
dataset <- dataset_clean7

#################################
## milestones

Q1 <- quantile(dataset$milestones, 0.25)
Q3 <- quantile(dataset$milestones, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
library(dplyr)
dataset_clean8 <- dataset %>%
  filter(milestones >= lower_bound, milestones <= upper_bound)

cat("milestones","\n")
cat("Original dataset dimensions: ", dim(dataset), "\n")
cat("Cleaned dataset dimensions: ", dim(dataset_clean8), "\n","\n")

dataset <- dataset_clean8
```
This code chunk removes outlier numerical values from a dataset, keeping only data points within the normal range in the dataset.

```{r}
nrow(dataset)
ncol(dataset)
```
After removing outliers, there are 724 rows and 49 columns remaining in the dataset. 

##### Numerical Attribute Boxplots After Removing Outliers:

```{r}
library(ggplot2)

# Select numerical attributes for boxplots
numerical_attributes <- c("latitude", "longitude",
                           "age_first_funding_year", "age_last_funding_year",
                           "age_first_milestone_year", "age_last_milestone_year",
                           "relationships", "funding_rounds", "funding_total_usd",
                           "milestones", "avg_participants")

# Melt the dataset for easier plotting
melted_data <- reshape2::melt(dataset[, numerical_attributes])

# Create boxplots with facet wrap
boxplot_plot <- ggplot(melted_data, aes(x = variable, y = value)) +
  geom_boxplot(fill = "darkgray", color = "black") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Boxplots for Numerical Attributes After Removing Outliers",
       x = "Attribute",
       y = "Value") +
  theme(plot.title = element_text(hjust = 0.5))

# Display the plot
print(boxplot_plot)

```
By removing the extreme values, the new range encompasses a larger spread of the data compared to the original range with outliers. The distribution appears more compact and less skewed. However, this change might not always lead to an increase in the actual data values or the total range but rather a change in the scale of the visualization due to the removal of the extreme values. <br><br>



## 4.2-Data Integration

In data integration we are creating a unified view by combining information from diverse sources. 
Since all of our data is contained in one dataset and we are not adding two or more columns together, there is no need for the data integration step. <br><br>



## 4.3-Data Reduction

In the data reduction step, we are enhancing efficiency and model performance by minimizing data size, focusing on relevant features, and mitigating the risk of overfitting. We will conduct the chi-squared test, check for duplication, and remove redundancy. <br>

### A) Elimination of Duplicates

#### Duplicate columns 

```{r}
names(dataset)
```
There are no duplicated columns as each attribute is unique. 

#### Duplicate rows 

```{r}
sum(duplicated(dataset))
```
There is one duplicated row in the dataset that we must eliminate. 


```{r}
duplicated(dataset)
```
The duplicated row is in row 653.

```{r}
dataset[653, ]
```
From the table we can find the company name of the duplicate row to verify its duplication. 

```{r}
redwood_systems_rows <- dataset[dataset$name == "Redwood Systems", ]
print(redwood_systems_rows)
```
There are two rows dedicated to "Redwood Systems". This shows that row 653 is indeed a duplicate. Therefore, we must eliminate it. 

##### Remove Duplicated Rows:
```{r}
dataset <- unique(dataset)
```
This code chunk removes duplicated rows. 

##### Verify the Elimination of  Duplicates:
```{r}
sum(duplicated(dataset))
```
The duplicated row is removed.<br>


### B) Chi-squared Test

The chi-square test aids in feature selection by examining the independence between categorical attributes. It helps eliminate attributes that showcase a significant relationship between one another. 

#### Relationship Between labels and status 
```{r}
contingency_table <- table(dataset$labels, dataset$status)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```
The performed Chi-squared test demonstrates a substantial association between the ‘labels’ and ‘status’ columns. The obtained p-value, which is less than 2e-16, suggests a **statistically significant correlation**. Status and labels are **dependent on one another. So, we can eliminate one of them**. We choose to eliminate labels and keep status which is the class label. Labels might be the encoded version of status. <br>

#### Relationship Between zip_code and city 
```{r}
contingency_table <- table(dataset$zip_code, dataset$city)
chi_square_result <- chisq.test(contingency_table)
print(chi_square_result)
```
This code chunk conducts the chi-squared test for attributes zip_code and city. With the results shown, we can interpret that zip_code and city have a **significant correlation** between them. In other words, **they are dependent on one another. So, we can eliminate one of them**. We chose to eliminate zip_code and keep city. Because city constitutes a part of the attribute Unnammed: 6, and if we want to delete column Unnamed: 6 we must keep city.<br>


#### C) Elimination of Redundant Attributes

##### Before Attribute Elimination:

```{r}
ncol(dataset)
```
There are 49 columns in the dataset. 

```{r}
names(dataset)
```
Unnamed: 0, latitude, longitude, zip_code, state_code.1, id, Unnamed: 6, and object_id are existing attributes in the dataset. 

The chi-squared test showed us that zip_code and city are highly correlated. As a result, we can delete one of them and keep the other. 

Attributes Unnamed: 0, id, Unnamed: 6, and object_id are irrelevant to this data mining task. As discussed in section 2, the original reference provides no justification on their usage. Therefore, they are unimportant and can be eliminated. 

Latitude and longitude attributes are "accessory" attributes with no real impact on the dataset. Deleting them will simplify the later steps.

State_code and state_code.1 are duplicates. Only one has to remain to prevent redundancy. 


| Attribute(s)                  | Keep               | Remove     | Why                                     |
| ----------------------------- | ------------------ | ---------- | --------------------------------------- |
| Unnamed: 0                    | don't keep         | Unnamed: 0 | Irrelevant                              |   
| state_code, state_code.1      | state_code         | state_code.1 | Duplicate attribute                   |
| latitude                      | don't keep         | latitude   | Unimportant                             |
| longitude                     | don't keep         | longitude  | Unimportant                             |
| zip_code, city                | city               | zip_code   | Dependent attributes (chi-squared test) |
| id                            | don't keep         | id         | Irrelevant                              |
| labels, status                | status             | labels     | Dependent attributes (chi-squared test) |
| Unnamed: 6, city, state_code.1| city, state_code.1 | Unnamed:6  | Redundant attribute                     |
| object_id                     | don't keep         | object_id  | Irrelevant                              |

##### Redundant Attribute Elimination:
```{r}
# Create a list of column names to remove
columns_to_remove <- c("Unnamed: 0", "state_code.1", "latitude", "longitude", "zip_code", "id", "Unnamed: 6", "labels", "object_id")

# Remove the specified columns from the dataset
dataset <- dataset[, !names(dataset) %in% columns_to_remove]
```
This code chunk removes all attributes redundant, irrelevant, and unimportant from the dataset.

##### After Attribute Elimination:

```{r}
ncol(dataset)
```
There are 40 columns in the dataset. 

```{r}
names(dataset)
```
Unnamed: 0, latitude, longitude, zip_code, state_code.1, id, Unnamed: 6, and object_id do not exist in the dataset. <br><br>


## 4.4-Data Transformation

In data transformation, we are preparing data for analysis and modeling through flooring, normalization, and encoding techniques. <br>

#### A) Flooring Attributes 

Flooring data, an essential aspect of data transformation, is crucial for various analytical and modeling processes. It facilitates the conversion of continuous numerical attributes into discrete values by rounding down to the nearest whole number. This technique is vital in simplifying complex numerical data, making it more manageable and easier to interpret.

##### Attributes Before Flooring:
```{r}
# Selecting the specific columns for the first row
first_row <- dataset[1, c("age_first_funding_year", "age_last_funding_year",
                          "age_first_milestone_year", "age_last_milestone_year",
                          "relationships", "funding_rounds", "funding_total_usd",
                          "milestones", "avg_participants")]
# Printing the first row
print(first_row)
```
As shown above, the attributes contain continuous values before flooring. 

##### Flooring: 
```{r}
# Columns to floor
cols_to_floor <- c("age_first_funding_year", "age_last_funding_year",
                   "age_first_milestone_year", "age_last_milestone_year",
                   "relationships", "funding_rounds", "funding_total_usd",
                   "milestones", "avg_participants")

# Applying floor to specified columns
dataset[cols_to_floor] <- lapply(dataset[cols_to_floor], floor)
```
This code chunk floors attributes from continuous to discrete numbers. 

##### Attributes After Flooring:
```{r}
# Selecting the specific columns for the first row
first_row <- dataset[1, c("age_first_funding_year", "age_last_funding_year",
                          "age_first_milestone_year", "age_last_milestone_year",
                          "relationships", "funding_rounds", "funding_total_usd",
                          "milestones", "avg_participants")]
# Printing the first row
print(first_row)
```
After flooring, the attributes are now discrete instead of continuous.<br><br>


#### B) Normalization 

We are going to normalize the numerical attribute funding_total_usd using min-max normalization. Numbers should fall between 0 and 1 (inclusive).<br> 

##### funding_total_usd Before Normalization: 
```{r}
# Selecting the specific columns for the first row
first_row <- dataset[1, c("funding_total_usd")]
# Printing the first row
print(first_row)
```
The table above shows an unnormalized value from the funding_total_usd attribute. <br>

##### Normalization: 
```{r}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
dataset$funding_total_usd<-normalize(dataset$funding_total_usd)
```
This code chunk normalizes attribute funding_total_usd using min-max normalization. <br>

##### funding_total_usd After Normalization:
```{r}
# Selecting the specific columns for the first row
first_row <- dataset[1, c("funding_total_usd")]
# Printing the first row
print(first_row)
```
The table above shows a normalized value from the funding_total_usd attribute. 

```{r}
min_value <- min(dataset$funding_total_usd)
max_value <- max(dataset$funding_total_usd)

# Print the results with labels
cat("The min is:", min_value, "\n")
cat("The max is:", max_value)
```
In the min-max normalization of attribute funding_total_usd, the minimum is 0 while the max is 1. 

```{r}
# Find row index for minimum and maximum funding_total_usd
min_row <- which.min(dataset$funding_total_usd)
max_row <- which.max(dataset$funding_total_usd)

# Print rows with minimum and maximum funding_total_usd along with name and status
print(dataset[min_row, c('name', 'funding_total_usd', 'status')])

print(dataset[max_row, c('name', 'funding_total_usd', 'status')])
```
In both tables, we can verify that the min-max normalization was successful. The first table shows the row with minimum normalization and the second table shows the row with maximum normalization.<br><br>

#### C) Encoding 

Here, we will encode attributes to simplify analysis. The attributes to encode are:

i) Date attributes: founded_at, closed_at, first_funding_at, last_funding_at

ii) Class label: status

iii) Categorical attributes: state_code, category_code, and city

iv) Unique attribute: name


##### Attributes Before Encoding:
```{r}
dataset[3, c("state_code", "city", "name", "founded_at", "closed_at", "first_funding_at", "last_funding_at", "category_code", "status")]
```
Attributes appearing in their original format. 


### i) Encoding Date Attributes: founded_at, closed_at, first_funding_at, last_funding_at
```{r}
dataset$founded_at <- gsub("/", "", dataset$founded_at)
dataset$closed_at <- gsub("/", "", dataset$closed_at)
dataset$first_funding_at <- gsub("/", "", dataset$first_funding_at)
dataset$last_funding_at <- gsub("/", "", dataset$last_funding_at)

dataset$founded_at <- substr(dataset$founded_at, nchar(dataset$founded_at) - 3, nchar(dataset$founded_at))
dataset$closed_at <- substr(dataset$closed_at, nchar(dataset$closed_at) - 3, nchar(dataset$closed_at))
dataset$first_funding_at <- substr(dataset$first_funding_at, nchar(dataset$first_funding_at) - 3, nchar(dataset$first_funding_at))
dataset$last_funding_at <- substr(dataset$last_funding_at, nchar(dataset$last_funding_at) - 3, nchar(dataset$last_funding_at))

dataset$founded_at <- as.numeric(dataset$founded_at)
dataset$closed_at <- as.numeric(dataset$closed_at)
dataset$first_funding_at <- as.numeric(dataset$first_funding_at)
dataset$last_funding_at <- as.numeric(dataset$last_funding_at)
```
This code chunk converts attributes founded_at, closed_at, first_funding_at, last_funding_at from date format mm/dd/yyyy to numerical numbers. 


### ii) Encoding Categorical Attributes: state_code, category_code, and city 
```{r}
library(caret)
library(dplyr)

# Columns to be encoded
attributes_to_encode <- c("state_code", "category_code", "city")

# Loop through each attribute for encoding
for (attribute in attributes_to_encode) {
  # Use caret's method for encoding
  dataset[[attribute]] <- as.factor(dataset[[attribute]])
  dataset[[attribute]] <- as.numeric(dataset[[attribute]])
}
```

This code chunk creates from each of the selected columns a new column that holds a frequency-encoded version of the attributes values. Rows that share the same value of the attribute have the same encoding. 


### iii) Ecoding Unique Attribute: name
```{r}
# Replace each unique category with a numerical label
dataset$name <- as.numeric(factor(dataset$name, levels = unique(dataset$name)))
```
Creates a new column for the encoded unique values of the attribute name. 


### iv) Encoding Class Label: status
```{r}
dataset$status <- ifelse(dataset$status == "acquired", 1, 0)
```
Encodes status attribute to 1 for acquired status and 0 for closed status. 


##### Attributes After Encoding:
```{r}
dataset[3, c("state_code", "city", "name", "founded_at", "closed_at", "first_funding_at", "last_funding_at", "category_code", "status")]
```
Attributes founded_at, closed_at, first_funding_at, and last_funding_at appear in the encoded ( mm-yyyy ) form. Attribute status appear in 1 (for acquired) and 0 (for closed) form. 


## Comparison of Attributes Before vs After Pre-processing

After preprocessing, the data undergoes transformation that can include cleaning, normalization, encoding, and more, resulting in a refined dataset more amenable to analysis, reducing noise, and enhancing the accuracy of the learningmodels.

First, we will check the number or columns and attributes after pre-processing. 

```{r}
# Number of columns
num_cols <- ncol(dataset)
# Number of attributes
num_attrs <- nrow(dataset)

# Print the values
cat("Number of columns:", num_cols, "\n")
cat("Number of attributes:", num_attrs, "\n")
```
There are 40 columns and 727 rows. 

Then, we will observe changes occurred to numerical, nominal, and binary attributes after pre-processing.<br>

### A) Assessing Class Label Balance After Pre-processing

```{r}
library(ggplot2)
library(grid)

# Create a bar plot for the "status" attribute
gg <- ggplot(dataset, aes(x = status)) +
  geom_bar(fill = "darkgray", color = "black") +
  labs(title = "Distribution of Startup Status", x = "Status", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))

# Print the plot
print(gg)

# Add an external annotation to the right side
grid.text("1 = Acquired\n0 = Closed", x = 0.2, y = 0.92, just = c("right", "top"), gp = gpar(fontsize = 12, col = "black"))
```
Despite pre-processing, **there is still a class imbalance in the class label (status)**. <br>


#### B) Numerical Attributes Before vs After Pre-processing

```{r}
# Load the required libraries
library(ggplot2)

# Select numerical attributes for histograms
numerical_attributes <- c("age_first_funding_year", "age_last_funding_year",
                           "age_first_milestone_year", "age_last_milestone_year",
                           "relationships", "funding_rounds", "funding_total_usd",
                           "milestones", "avg_participants")

# Melt the dataset for easier plotting
melted_data <- reshape2::melt(dataset[, numerical_attributes])

# Create histogram with facet wrap
histogram_plot_numerical_after <- ggplot(melted_data, aes(x = value)) +
  geom_histogram(binwidth = 1, fill = "darkgray", color = "black") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Histogram for Numerical Attributes AFTER Pre-processing", x = "Value", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5))

# Set up a 1x2 plotting grid to show histograms side by side
par(mfrow = c(1, 2))

# Plot the histograms side by side
plot(histogram_plot_numerical_before)
plot(histogram_plot_numerical_after)
```
The graphs above show a comparison between numerical data BEFORE and AFTER pre-processing. Many alterations have taken place within the numerical attributes. From flooring, normalization, and removing outliers. <br>

#### C) Nominal Attributes Before vs After Pre-processing

```{r}
library(ggplot2)
library(tidyr)

# Selecting specific columns for visualization
columns_to_visualize <- dataset[, c("state_code", "city", "category_code")]

# Melt the required columns for visualization
melted_data_before <- gather(data = columns_to_visualize)

# Plotting bar graphs for the specified attributes and facet_wrap
barplot_nominal_after <- ggplot(melted_data_before, aes(x = value, fill = key)) +
  geom_bar(position = "dodge", stat = "count", color = "black", fill = "darkgray") +
  facet_wrap(~key, scales = "free") +
  labs(title = "Nominal Attributes AFTER Pre-processing", x = "Values", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))

# Displaying the plot
print(barplot_nominal_before)
print(barplot_nominal_after)
```
Many alterations have taken place within the nominal attributes. Attributes were encoded and outliers were removed. In addition, attribute state_code.1 was deemed redundant and hence removed. From the graphs, we can observe frequent trends in category_code, city, and state_code. <br>


#### D) Binary Attributes Before vs After Pre-processing

```{r}
library(ggplot2)
library(tidyr)

# List of binary attributes
binary_attributes <- c("is_CA", "is_NY", "is_MA", "is_TX", "is_otherstate", 
                       "is_software", "is_web", "is_mobile", "is_enterprise", 
                       "is_advertising", "is_gamesvideo", "is_ecommerce", 
                       "is_biotech", "is_consulting", "is_othercategory", 
                       "has_VC", "has_angel", "has_roundA", "has_roundB", 
                       "has_roundC", "has_roundD", "is_top500", "status")

# Melt the datasets for easier plotting
melted_data <- gather(dataset, key = "variable", value = "value", all_of(binary_attributes))

# Create bar plots with facet wrap
barplot_binary_after <- ggplot(melted_data, aes(x = value, fill = variable)) +
  geom_bar(position = "dodge", stat = "count", color = "black", fill = "darkgray") +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Bar Plots for Binary Attributes AFTER Pre-processing", x = "Value", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))

# Display the plot
print(barplot_binary_before)
print(barplot_binary_after)
```
 The binary attributes have remained unaltered; no changes have been applied.<br><br>

## Plotting of Pre-processed Data 

Plotting pre-processed data is crucial as it visually unveils patterns, trends, and distributions within the dataset. It helps understand attribute distributions, correlations, and spot variations post-preprocessing, which is fundamental for making informed decisions and uncovering insights in the data analysis process.<br>

### A) Relationship between Top 500 Companies and Status

```{r}
cor(dataset$is_top500, dataset$status)
```
A correlation coefficient of 0.315 suggests a moderate positive correlation between the attributes ‘is_top500’ and the class label ‘status’. This implies that changes in one variable are associated with relatively proportional changes in the other variable, albeit not perfectly.

```{r}
library(ggplot2)

# Create a summary table to count the combinations of is_top500 and status
summary_table <- table(dataset$is_top500, dataset$status)

# Convert the summary table to a data frame
summary_df <- as.data.frame(summary_table)

# Rename the columns for clarity
colnames(summary_df) <- c("is_top500", "status", "count")

# Create a barplot
ggplot(summary_df, aes(x = is_top500, y = count, fill = status)) +
  geom_bar(stat = "identity", position = "dodge", aes(fill = status), color = "black") +
  labs(title = "Top 500 Company vs. Status", x = "Top 500", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("acquired" = "darkgray", "closed" = "darkgray"))
```
**Is being a Top 500 company a strong indicator of whether a startup will be acquired or closed?** The vast majority of top 500 companies are acquired companies.<br>


### B) Distribution of Startup Sectors

```{r}
library(ggplot2)
library(scales)

# Create a data frame for your binary attributes
binary_data <- data.frame(
  Attribute = c("is_software", "is_web", "is_mobile", "is_enterprise", "is_advertising", "is_gamesvideo", "is_ecommerce", "is_biotech", "is_consulting", "is_othercategory"),
  Value = c(sum(dataset$is_software), sum(dataset$is_web), sum(dataset$is_mobile), sum(dataset$is_enterprise), sum(dataset$is_advertising), sum(dataset$is_gamesvideo), sum(dataset$is_ecommerce), sum(dataset$is_biotech), sum(dataset$is_consulting), sum(dataset$is_othercategory))
)

# Calculate percentages
binary_data$Percentage <- (binary_data$Value / sum(binary_data$Value)) * 100

# Create the pie chart
pie_chart <- ggplot(binary_data, aes(x = "", y = Percentage, fill = Attribute)) +
  geom_bar(stat = "identity") +
  coord_polar(theta = "y") +
  labs(title = "Startup Category") +
  scale_y_continuous(labels = percent_format(scale = 1))

# Display the pie chart
print(pie_chart)
```

**What are the most popular startup sectors?** The answer is software. Most startups focus on tech-related sectors, like software, web, and mobile.<br>


### C) Distribution of Startup State of Origin

```{r}
library(ggplot2)

# Create a data frame with the counts of each binary attribute
binary_data <- data.frame(
  Attribute = c("is_CA", "is_NY", "is_MA", "is_TX", "is_otherstate"),
  Count = c(
    sum(dataset$is_CA),
    sum(dataset$is_NY),
    sum(dataset$is_MA),
    sum(dataset$is_TX),
    sum(dataset$is_otherstate)
  )
)

# Calculate percentages
binary_data$Percentage <- (binary_data$Count / sum(binary_data$Count)) * 100

# Create a pie chart
pie_chart <- ggplot(binary_data, aes(x = "", y = Percentage, fill = Attribute)) +
  geom_bar(stat = "identity", width = 1, color="black") +
  coord_polar(theta = "y") +  # Convert to polar coordinates for a pie chart
  labs(title = "Distribution of Startup State of Origin") +
  scale_fill_manual(values = c("is_CA" = "lightgray", "is_NY" = "darkgray", "is_MA" = "darkgray", "is_TX" = "darkgray", "is_otherstate" = "darkgray")) +
  theme_minimal() +  
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), position = position_stack(vjust = 0.5))

# Display the pie chart
print(pie_chart)
```
**Which state is the most popular choice for startups to launch in?** The answer is California. 51.4% of all startups launched from California.<br>

### D) Distribution of Funding Rounds and Status

```{r}
library(ggplot2)
library(cowplot)
library(tidyr)  # Load the tidyr package

# Filter rows where status is "acquired"
acquired_data <- subset(dataset, status == "1")

# Create a long-format dataset for use with ggplot2
acquired_data_long <- tidyr::gather(acquired_data, key = "Attribute", value = "BinaryValue", has_VC, has_angel, has_roundA, has_roundB, has_roundC, has_roundD)

# Create histograms with facet_wrap for "acquired" status
plot1 <- ggplot(acquired_data_long, aes(x = BinaryValue)) +
  geom_histogram(binwidth = 1, fill = "darkgray", color = "black") +
  facet_wrap(~ Attribute, scales = "free_x") +
  labs(title = "'Acquired' Status", x = "Value", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5))

# Filter rows where status is "closed"
closed_data <- subset(dataset, status == "0")

# Create a long-format dataset for use with ggplot2
closed_data_long <- tidyr::gather(closed_data, key = "Attribute", value = "BinaryValue", has_VC, has_angel, has_roundA, has_roundB, has_roundC, has_roundD)

# Create histograms with facet_wrap for "closed" status
plot2 <- ggplot(closed_data_long, aes(x = BinaryValue)) +
  geom_histogram(binwidth = 1, fill = "darkgray", color = "black") +
  facet_wrap(~ Attribute, scales = "free_x") +
  labs(title = "'Closed' Status", x = "Value", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5))

# Plotting the two histograms side by side
plot_grid(plot1, plot2, nrow = 1)
```
**Do increased funding rounds serve as an indicator of whether a startup is more likely to be acquired or closed in the future?** The answer is not necessarily, but from the graphs we can see that the number of acquired startups that went through series A, B, C, and D is more than the number of closed startups that went through them. The majority of "acquired" startups had a series A funding round, while the majority of "closed" startups did not. Series A and Series B funding could be a potential indicator of startup success. Angel investors invested almost equally in both "acquired" and "closed" startups. VCs tend to invest more in startups that become "acquired" in the future. Nevertheless, the results may be inaccurate because the class label is imbalanced.<br>


### E) Relationship Between the Number of Funding Rounds and Total Funding Raised

```{r}
range(dataset$funding_total_usd)
```
Attribute funding_total_usd was normalized using min-max normalization. So, the range of funding_total_usd values fall within 0 (minimum) and 1 (maximum). 

```{r}
range(dataset$funding_rounds)
```
The least amount of funding rounds went through by any startup is 1, indicating that all startups at least went through one funding round. The most amount of funding rounds went through by any startup is 6. 

```{r}
cor(dataset$funding_rounds, dataset$funding_total_usd)
```
A correlation of 0.439 indicates a moderate positive correlation between the variables 'funding_rounds' and 'funding_total_usd'.

```{r}
boxplot(dataset$funding_total_usd ~ dataset$funding_rounds,
        xlab = "Number of Funding Rounds",
        ylab = "Funding Total (USD)",
        main = "Funding Rounds vs Funding Total",
        col = "darkgray")
```
**Is there a strong correlation between the number of funding rounds and the total funding received by a company?** It’s generally true, but not an absolute rule. There’s often a positive relationship between increased funding rounds and raised funds, indicating that more rounds tend to yield more money.<br>


### F) Association Between Total Funding and Startup Category

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)

# Summary table: Sum of funding for each category
category_funding_summary <- dataset %>%
  group_by(is_software, is_web, is_mobile, is_enterprise, 
           is_advertising, is_gamesvideo, is_ecommerce, is_biotech, is_consulting, is_othercategory) %>%
  summarise(total_funding = sum(funding_total_usd))

# Reshape the data for plotting
category_funding_summary <- category_funding_summary %>%
  pivot_longer(
    cols = -total_funding,
    names_to = "Category",
    values_to = "BinaryValue"
  )

# Create a bar plot
ggplot(category_funding_summary, aes(x = Category, y = total_funding, fill = BinaryValue)) +
  geom_bar(stat = "identity") +
  labs(title = "Total Funding vs Startup Category", x = "Category", y = "Total Funding") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```
**Is there a strong correlation between the sector of the company and the total funding received by a company?** The answer is yes. Software startups received the highest amounts of funding. Followed by other tech-related sectors: web and mobile.<br><br><br>


## 5-Data Mining Techniques

To identify patterns and make predictions based on the data of the dataset, we are going to perform the data mining techniques classification and clustering. **Classification** is a supervised learning technique that uses labeled data to train a model to predict the class of new data points. **Clustering** is an unsupervised learning technique that groups unlabeled data points into clusters based on their similarities. <br><br>


### 5.1-Classification 

As mentioned before, **classification is a form of supervised learning, where the class label is known beforehand**. In the case of startup data, the class label is the binary attribute "status". Status attribute holds two values, either 1 for "acquired" status, or 0 for "closed" status. In classification, the algorithm learns from labeled training data, then establishes a relationship between input features and their respective classes. The learned patterns are then used to classify new, unseen data. 

In this project, we are going to use the **decision tree** algorithm to perform classification. Decision tree is considered a greedy algorithm. The tree is constructed in a top-down recursive divide-and-conquer manner. It takes the form of a branching tree; the top node is the decision node and the bottom unbranched leaves represent the class. 

The following classification steps will be performed:

1. Construct a model using training data. 
2. Evaluate the model using testing data. 
3. Predict the class using new data.

#### Partitioning Method

**We will use the train:test split method as the partitioning method. The dataset will be partitioned into 70% training and 30% testing, 80% training and 20% testing and finally 90% training and 10% testing.** Train:test split method is a common practice in machine learning. It helps in maximizing the usage of available data for both training and testing. By performing this  method, we can prevent an overfitted model by providing a separate data set for model evaluation.

Using the train:test split method, we will try to section data into 70:30 data split, 80:20 data splits, and 90:10 data splits. For each split we will run Information Gain, Gain Ration, and Gini Index. Ultimately, we will have a total of nine decision trees.

**◆ Information Gain** is is a measure used in the decision tree algorithm. It represents the amount of disorder or uncertainty in a dataset that is reduced (or gained) after splitting the data on an attribute. It measures the reduction in entropy (impurity or disorder) after the dataset is split. Attribute with the **maximum information gain is selected as the decision node**. 

**◆ Gain Ratio** is considers the Information Gain but normalizes it by the intrinsic information associated with each split. It accounts for the size of the branches resulting from a split. Attribute with the maximum gain ratio is selected as the splitting attribute. Attribute with the **maximum gain ratio is selected as the splitting attribute**.

**◆ Gini Index** is is a measure of impurity or the quality of a split in a dataset. In the context of decision trees, it calculates the overall probability of a specific feature being misclassified. Attribute with the **minimum gini index is selected as the splitting attribute**.

Before we start let's take a look at the data after pre-processing:

```{r}
str(dataset)
dataset <- read_excel("preprocessed_dataset.xlsx")
```
All categorical attributes have been changed to a numerical format to ensure smooth utilization of classification tree plotting functions.

##### Load necessary packages:
```{r}
install.packages("partykit")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("ROSE")
install.packages("caret")
install.packages("C50")
library(party)
library(partykit)
library(rpart.plot)
library(RWeka)
library(caret)
library(rpart)
library(ggplot2)
library(lattice)
```
### Balancing Class Label

```{r}
# Identifying the rows of each status
closed_indices <- which(dataset$status == 0)
acquired_indices <- which(dataset$status == 1)

# Number of samples for each status class
num_closed <- length(closed_indices)
num_acquired <- length(acquired_indices)

# Subsampling to balance the data
if (num_closed > num_acquired) {
  # Subsample the "closed" status to match the "acquired" count
  sampled_closed_indices <- sample(closed_indices, num_acquired)
  balanced_dataset <- rbind(dataset[sampled_closed_indices, ], dataset[acquired_indices, ])
} else {
  # Subsample the "acquired" status to match the "closed" count
  sampled_acquired_indices <- sample(acquired_indices, num_closed)
  balanced_dataset <- rbind(dataset[closed_indices, ], dataset[sampled_acquired_indices, ])
}
```
We have previously shown the imbalance of the class label "status" (section 4: Assessing Class Label Balance After Pre-processing). Now, we have to balance it to prevent the model from becoming biased towards the majority and thus produce inaccurate results. 

##### Verify Class Label Balance 
```{r}
# Count of each class in the original dataset
original_class_counts <- table(dataset$status)

# Count of each class in the balanced dataset
balanced_class_counts <- table(balanced_dataset$status)

# Display the counts
print("Original Dataset Class Counts:")
print(original_class_counts)
print("Balanced Dataset Class Counts:")
print(balanced_class_counts)
```
There are 452 rows for acquired status (majority), and 275 rows for closed status (minority) before balancing. 
After balancing, there are 275 rows for acquired status and 275 rows for closed status showcasing a balnced class label ready for classification. 


#### Model Evaluation Metrics
```{r}
# Function for Model Evaluation
evaluate_model <- function(predictions, actual_labels) {
  # Confusion Matrix
  confusion_matrix <- table(actual_labels, predictions)
  print(confusion_matrix)
  
  TP <- confusion_matrix[1, 1]
  TN <- confusion_matrix[2, 2]
  FP <- confusion_matrix[2, 1]
  FN <- confusion_matrix[1, 2]
  
  # Calculate metrics
  accuracy <- ((TP + TN) / sum(confusion_matrix)) * 100
  precision <- (TP / (TP + FP)) * 100
  sensitivity <- (TP / (TP + FN)) * 100
  specificity <- (TN / (TN + FP)) * 100
  
  # Print metrics
  cat("Accuracy:", accuracy, "%\n")
  cat("Precision:", precision, "%\n")
  cat("Sensitivity (Recall):", sensitivity, "%\n")
  cat("Specificity:", specificity, "%\n")
  
  # Return a list of metrics
  return(list(
    accuracy = accuracy,
    precision = precision,
    sensitivity = sensitivity,
    specificity = specificity
  ))
}

install.packages(c("caret", "pROC"))
library(caret)
library(pROC)
```


## 1-First Partition: (70% training, 30% testing)

### Partitioning 
```{r}
set.seed(1234)
ind=sample(2, nrow(balanced_dataset), replace=TRUE, prob=c(0.70 , 0.30))
train_data_70=balanced_dataset[ind==1,]
test_data_70=balanced_dataset[ind==2,]
```
Splits the dataset into 70% training data and 30% testing data. 

#### Verify Partitioning
```{r}
dim(train_data_70)
dim(test_data_70)
```
The training data consist of 381 rows. 
The testing data consist of 169 row.


### A) ID3 Algorithm: Information Gain 

### A.1) Constructing the Decision Tree 
```{r}
library(party)
myFormula <- status~ state_code + city + name + founded_at + closed_at + first_funding_at + last_funding_at + 
            age_first_funding_year + age_last_funding_year + age_first_milestone_year + age_last_milestone_year + 
            relationships + funding_rounds + funding_total_usd + milestones + is_CA + is_NY + is_MA + is_TX + is_otherstate + 
            category_code + is_software + is_web + is_mobile + is_enterprise + is_advertising + is_gamesvideo + is_ecommerce + 
            is_biotech + is_consulting + is_othercategory + has_VC + has_angel + has_roundA + has_roundB + has_roundC + 
            has_roundD + avg_participants + is_top500

dataset_ctree_IG_70<-ctree(myFormula, data=train_data_70)

table(predict(dataset_ctree_IG_70), train_data_70$status)
```
This code builds the decision tree model. 

#### A.2) Plotting
```{r}
print(dataset_ctree_IG_70)
plot(dataset_ctree_IG_70)
plot(dataset_ctree_IG_70,type="simple")
```
Presented are two tree models: simplified and original. In decision tree models, the attribute demonstrating the most significant information gain functions as the decision node or the root. Upon inspection of the displayed trees, it’s apparent that the ‘relationships’ attribute possesses the highest information gain, positioned prominently at the top of the tree structure.

### A.3) Testing 
```{r}
predictions_IG_70 <- predict(dataset_ctree_IG_70, newdata = test_data_70, type = "response")
labels_IG_70 <- test_data_70$status
```
The code chunk predicts the value of the class label 'status' based on test data. It provides a contingency table comparing the predicted values against the actual status. 


### A.4) Confusion Matrix 
```{r}
confusion_matrix_IG_70 <- table(test_data_70$status, predictions_IG_70)
print(confusion_matrix_IG_70)
```
### A.5) Evaluation Metrics

```{r}
metrics_IG_70 <- evaluate_model(predictions_IG_70, labels_IG_70)
print(metrics_IG_70)
```
### B) C50 Algorithm: Gain Ratio

### B.1) Constructing the Decision Tree 

```{r}
library(C50)

# Convert 'status' to a factor
train_data_70$status <- as.factor(train_data_70$status)

# Train the model using C5.0 with Gain Ratio for splitting
C5Fit_70 <- C5.0(status ~ ., data = train_data_70, control = C5.0Control(earlyStopping = FALSE, CF = 0.25))

# Print the summary of the model
summary(C5Fit_70)
```

### B.2) Plotting

```{r}
plot(C5Fit_70, type="simple")
```

### B.3) Testing 

```{r}
predictions_GR_70 <- predict(C5Fit_70, newdata = test_data_70)
labels_GR_70 <- test_data_70$status
```
The code chunk predicts the value of the class label 'status' based on test data. It provides a contingency table comparing the predicted values against the actual status. 


### B.4) Confusion Matrix 

```{r}
# Create a confusion matrix
confusion_matrix_GR_70 <- table(test_data_70$status, predictions_GR_70)

# Display the confusion matrix
print(confusion_matrix_GR_70)
```
### B.5) Evaluation Metrics

```{r}
metrics_GR_70 <- evaluate_model(predictions_IG_70, labels_GR_70)
print(metrics_GR_70)
```

### CART Algorithm: Gini Index 

### C.1) Constructing the Decision Tree 
```{r}
library(rpart)
# Gini index (CART) and Tree model 70:30
dataset_ctree_GI_70 <- rpart(status ~ ., data = train_data_70, method = "class", parms = list(split = "gini"))
```

### C.2) Plotting
```{r}
library(rpart.plot)
print(dataset_ctree_GI_70)
rpart.plot(dataset_ctree_GI_70)
```

### C.3) Testing 
```{r}
predictions_GI_70 <- predict(dataset_ctree_GI_70, newdata = test_data_70, type = "class")
labels_GI_70 <- test_data_70$status
```

### C.4) Confusion Matrix
```{r}
# Create a confusion matrix
confusion_matrix_GI_70 <- table(test_data_70$status, predictions_GI_70)

# Display the confusion matrix
print(confusion_matrix_GI_70)
```
### C.5) Evaluation Metrics

```{r}
metrics_GI_70 <- evaluate_model(predictions_GI_70, labels_GI_70)
```

--------------------------------------------------------------------------------

## Second Partition:  (80% training, 20% testing)

### Partitioning 
```{r}
set.seed(1234)
ind=sample(2, nrow(balanced_dataset), replace=TRUE, prob=c(0.80 , 0.20))
train_data_80=balanced_dataset[ind==1,]
test_data_80=balanced_dataset[ind==2,]
```

#### Verify Partitioning
```{r}
dim(train_data_80)
dim(test_data_80)
```

#### A.1) Constructing the Decision Tree
```{r}
library(party)
myFormula <- status ~ state_code + city + name + founded_at + closed_at + first_funding_at + last_funding_at + 
            age_first_funding_year + age_last_funding_year + age_first_milestone_year + age_last_milestone_year + 
            relationships + funding_rounds + funding_total_usd + milestones + is_CA + is_NY + is_MA + is_TX + is_otherstate + 
            category_code + is_software + is_web + is_mobile + is_enterprise + is_advertising + is_gamesvideo + is_ecommerce + 
            is_biotech + is_consulting + is_othercategory + has_VC + has_angel + has_roundA + has_roundB + has_roundC + 
            has_roundD + avg_participants + is_top500
dataset_ctree_IG_80 <- ctree(myFormula, data = train_data_80)

table(predict(dataset_ctree_IG_80), train_data_80$status)
```


#### A.2) Plotting
```{r}
print(dataset_ctree_IG_80)
plot(dataset_ctree_IG_80)
plot(dataset_ctree_IG_80, type = "simple")
```


#### A.3) Testing
```{r}
predictions_IG_80 <- predict(dataset_ctree_IG_80, newdata = test_data_80, type = "response")
labels_IG_80 <- test_data_80$status
```

#### A.4) Confusion Matrix
```{r}
confusion_matrix_IG_80 <- table(test_data_80$status, predictions_IG_80)
print(confusion_matrix_IG_80)
```

#### A.5) Evaluation Metrics
```{r}
metrics_IG_80 <- evaluate_model(predictions_IG_80, labels_IG_80)
print(metrics_IG_80)
```


#### B) C50 Algorithm: Gain Ratio

#### B.1) Constructing the Decision Tree

```{r}
library(C50)
train_data_80$status <- as.factor(train_data_80$status)

C5Fit_80 <- C5.0(status ~ ., data = train_data_80, control = C5.0Control(earlyStopping = FALSE, CF = 0.25))
summary(C5Fit_80)
```


#### B.2) Plotting
```{r}
plot(C5Fit_80, type = "simple")
```


#### B.3) Testing
```{r}
predictions_GR_80 <- predict(C5Fit_80, newdata = test_data_80)
labels_GR_80 <- test_data_80$status
```


#### B.4) Confusion Matrix
```{r}
confusion_matrix_GR_80 <- table(test_data_80$status, predictions_GR_80)
print(confusion_matrix_GR_80)
```


#### B.5) Evaluation Metrics
```{r}
metrics_GR_80 <- evaluate_model(predictions_GR_80, labels_GR_80)
```


#### CART Algorithm: Gini Index

#### C.1) Constructing the Decision Tree
```{r}
library(rpart)
dataset_ctree_GI_80 <- rpart(status ~ ., data = train_data_80, method = "class", parms = list(split = "gini"))
```

#### C.2) Plotting
```{r}
print(dataset_ctree_GI_80)
rpart.plot(dataset_ctree_GI_80)
```

#### C.3) Testing
```{r}
predictions_GI_80 <- predict(dataset_ctree_GI_80, newdata = test_data_80, type = "class")
labels_GI_80 <- test_data_80$status
```

#### C.4) Confusion Matrix
```{r}
confusion_matrix_GI_80 <- table(test_data_80$status, predictions_GI_80)
print(confusion_matrix_GI_80)
```

#### C.5) Evaluation Metrics
```{r}
metrics_GI_80 <- evaluate_model(predictions_GI_80, labels_GI_80)
```



--------------------------------------------------------------------------------

### Third Partition:  (90% training, 10% testing)

####Partitioning
```{r}
set.seed(1234)
ind <- sample(2, nrow(balanced_dataset), replace=TRUE, prob=c(0.90, 0.10))
train_data_90 <- balanced_dataset[ind == 1,]
test_data_90 <- balanced_dataset[ind == 2,]
```

#### Verify Partitioning
```{r}
dim(train_data_90)
dim(test_data_90)
```

#### A.1) Constructing the Decision Tree
```{r}
library(party)
myFormula <- status ~ state_code + city + name + founded_at + closed_at + first_funding_at + last_funding_at + 
            age_first_funding_year + age_last_funding_year + age_first_milestone_year + age_last_milestone_year + 
            relationships + funding_rounds + funding_total_usd + milestones + is_CA + is_NY + is_MA + is_TX + is_otherstate + 
            category_code + is_software + is_web + is_mobile + is_enterprise + is_advertising + is_gamesvideo + is_ecommerce + 
            is_biotech + is_consulting + is_othercategory + has_VC + has_angel + has_roundA + has_roundB + has_roundC + 
            has_roundD + avg_participants + is_top500
dataset_ctree_IG_90 <- ctree(myFormula, data = train_data_90)

table(predict(dataset_ctree_IG_90), train_data_90$status)
```

#### A.2) Plotting
```{r}
print(dataset_ctree_IG_90)
plot(dataset_ctree_IG_90)
plot(dataset_ctree_IG_90, type = "simple")
```

#### A.3) Testing
```{r}
predictions_IG_90 <- predict(dataset_ctree_IG_90, newdata = test_data_90, type = "response")
labels_IG_90 <- test_data_90$status
```

#### A.4) Confusion Matrix
```{r}
confusion_matrix_IG_90 <- table(test_data_90$status, predictions_IG_90)
print(confusion_matrix_IG_90)
```

####  A.5) Evaluation Metrics
```{r}
metrics_IG_90 <- evaluate_model(predictions_IG_90, labels_IG_90)
print(metrics_IG_90)
```


## B) C50 Algorithm: Gain Ratio

## B.1) Constructing the Decision Tree
```{r}
library(C50)
train_data_90$status <- as.factor(train_data_90$status)

C5Fit_90 <- C5.0(status ~ ., data = train_data_90, control = C5.0Control(earlyStopping = FALSE, CF = 0.25))
summary(C5Fit_90)
```

## B.2) Plotting
```{r}
plot(C5Fit_90, type = "simple")
```

## B.3) Testing
```{r}
predictions_GR_90 <- predict(C5Fit_90, newdata = test_data_90)
labels_GR_90 <- test_data_90$status
```

## B.4) Confusion Matrix
```{r}
confusion_matrix_GR_90 <- table(test_data_90$status, predictions_GR_90)
print(confusion_matrix_GR_90)
```

## B.5) Evaluation Metrics
```{r}
metrics_GR_90 <- evaluate_model(predictions_GR_90, labels_GR_90)
```

## C) CART Algorithm: Gini Index

## C.1) Constructing the Decision Tree

```{r}
library(rpart)
dataset_ctree_GI_90 <- rpart(status ~ ., data = train_data_90, method = "class", parms = list(split = "gini"))
```

## C.2) Plotting
```{r}
print(dataset_ctree_GI_90)
rpart.plot(dataset_ctree_GI_90)
```


## C.3) Testing
```{r}
predictions_GI_90 <- predict(dataset_ctree_GI_90, newdata = test_data_90, type = "class")
labels_GI_90 <- test_data_90$status
```


## C.4) Confusion Matrix
```{r}
confusion_matrix_GI_90 <- table(test_data_90$status, predictions_GI_90)
print(confusion_matrix_GI_90)
```


## C.5) Evaluation Metrics
```{r}
metrics_GI_90 <- evaluate_model(predictions_GI_90, labels_GI_90)
```
70:30 Evaluation
                               | Accuracy | Precision | Sensitivity | Specificity 
ID3 Algorithm: Information Gain|  8.87%   |   93.75%  |    62.5%    |    0%
C50 Algorithm: Gain Ratio      |  8.87%   |   93.75%  |    62.5%    |    0%
CART Algorithm: Gini Index     |  75.15%  |   73.6%   |    69.7%    |    79.5%

80:20 Evaluation
                               | Accuracy | Precision | Sensitivity | Specificity 
ID3 Algorithm: Information Gain|  16.82%  |    100%   |    31.25%   |    100%
C50 Algorithm: Gain Ratio      |  73.8%   |    71.7%  |    68.75%   |    77.9%
CART Algorithm: Gini Index     |  77.57%  |    76.01% |    72.92%   |    81.35%

90:10 Evaluation
                               | Accuracy | Precision | Sensitivity | Specificity 
ID3 Algorithm: Information Gain|  6.34%   |   100%    |    66.67%   |    NaN%
C50 Algorithm: Gain Ratio      |  74.6%   |   66.6%   |    76.92%   |    72.97%
CART Algorithm: Gini Index     |  76.19   |  70.37%   |    73.07%   |    78.37%

Information Gain:

Information Gain is a measure of the reduction in entropy or impurity achieved by selecting a particular feature for splitting the data. It aims to select features that provide the most information about the target variable.
Gain Ratio:

Gain Ratio is an extension of Information Gain that takes into account the intrinsic information of the feature. It aims to penalize features that have a large number of distinct values, reducing the bias towards features with many categories.
Gini Index:

Gini Index is a measure of impurity in a set of data. In the  decision trees, it is used to evaluate how well a particular feature separates the data into different classes. A lower Gini Index indicates a better separation.
The choice of feature selection criteria can impact the structure of the decision tree and, consequently, the performance of the classifier. Here's why you might observe differences in accuracy and precision:

Different Splitting Criteria: Each of these metrics uses a different criterion for selecting the best feature to split the data at each node. This can lead to different decisions on how to partition the data, affecting the resulting tree structure.

Overfitting vs. Underfitting: The choice of feature selection criteria can influence the trade-off between overfitting and underfitting. Information Gain tends to favor more complex trees, while Gain Ratio penalizes complex trees. Gini Index, like Information Gain, tends to favor more complex trees.

Handling Categorical and Numeric Features: Information Gain and Gain Ratio are more suitable for categorical features, while Gini Index is often used for both categorical and numeric features. The nature of oour features might impact the performance of the criteria.

Data Characteristics: The effectiveness of different criteria can also depend on the characteristics of oour dataset, including the distribution of classes, the presence of noise, and the degree of feature interaction.

It's essential to experiment with different feature selection criteria and evaluate their impact on the performance of our decision tree classifier which is training:testing split method using appropriate evaluation metrics, such as accuracy, precision, recall, or specificity. The goal is to choose the criteria that lead to the best overall performance on the data set.


### 5.2-Clustering 
#We used these packages below for Clustering.
```{r}
install.packages(dplyr)
install.packages(ClusterR) 
install.packages(cluster) # To make clusters
install.packages(factoextra) #To visualize and validate the clusters
```

On contrary to classification, **clustering is a form of unsupervised learning, where there are no predefined classes**. However, in the start-up data, the class attribute is already known which is the "status". Status attribute holds two values, either "acquired" status, or "closed" status. If the class attribute is known, then what is the use clustering? It is still beneficial for exploratory data analysis to discover the unseen structure or pattern in the data set, findings of anomalies, visualizing the data set and even assessing the quality of the clustering algorithm.

We will use k-means as our partitioning method. The main goal is to partition into k numbers of clusters, where each data point belongs to the cluster with the nearest means. We chose k= 2, 3 and 4 to see what are the differences in the patterns of each k clusters and why their qualities differ from each other.
```{r}
dataset <- read_excel("preprocessed_dataset.xlsx")
```

```{r}
# First, transfer the columns from dataset to another dataset (dataset.features) to make it easier to cluster 
library(dplyr)
dataset.features = dataset %>% select(age_first_funding_year,age_last_funding_year,age_first_milestone_year,age_last_milestone_year,relationships,funding_rounds,funding_total_usd,milestones,has_VC,has_angel,has_roundA,has_roundB,has_roundC,has_roundD,avg_participants,is_top500)
View(dataset.features)

# Run k-means to find different number of clusters after omitting NA

library(ClusterR)
library(cluster)
set.seed(500)
kmeanResults<-kmeans(na.omit(dataset.features), 2)
kmeanResults

kmeanResults2<-kmeans(na.omit(dataset.features), 3)
kmeanResults2

kmeanResults3<-kmeans(na.omit(dataset.features), 4)
kmeanResults3

#Visualize the clusters 

library(factoextra)
fviz_cluster(kmeanResults, data = na.omit(dataset.features))

fviz_cluster(kmeanResults2, data = na.omit(dataset.features))

fviz_cluster(kmeanResults3, data = na.omit(dataset.features))
```
## 6-Evaluation and Comparison 

### 6.2-Clustering 

Silhouette method is an unsupervised method to assess the quality of the clusters. 
If silhouette method is:
- closer to 1, it shows how well a data point is in its own cluster and far away from neighboring clusters
- around 0, it shows that the cluster is overlapping with another
- closer to -1, it shows that a data point might be assigned to a wrong cluster.
```{r}
#Silhouette method
#For 2-mean cluster
silhouette_scores<-silhouette(kmeanResults$cluster, dist(na.omit(dataset.features)))
plot(silhouette_scores)

#For 3-mean cluster
silhouette_scores<-silhouette(kmeanResults2$cluster, dist(na.omit(dataset.features)))
plot(silhouette_scores)

#For 4-mean cluster
silhouette_scores<-silhouette(kmeanResults3$cluster, dist(na.omit(dataset.features)))
plot(silhouette_scores)

fviz_nbclust(na.omit(dataset.features), kmeans, method = "silhouette")
```

#The highest silhouette width indicates the optimal number of clusters because it is the least overlapping. 

Total Within-Cluster is found to be plotted on a graph to find the optimal numbers of cluster where the turning point is. This method is called an Elbow Method.
```{r}
#Total Within-cluster sum of squares
kmeanResults$tot.withinss
kmeanResults2$tot.withinss
kmeanResults3$tot.withinss

#Elbow method found by plotting tot.withinss
fviz_nbclust(na.omit(dataset.features), kmeans, method = "wss")
```
###BCubed Precision and Recall.
Precision indicates the purity of a cluster. The higher the precision, the higher the purity of the cluster. 
Recall indicates how good the data points of same true class are put into same cluster. 
```{r}

#Add status to Data.features
dataset.features=bind_cols(dataset.features,dataset['status'])

#Omitting NA rows since we clustered with NA values
dataset.features=na.omit(dataset.features)

#Find BCubed Precision and Recall
#1- Find the number of all the rows with acquired in the class by filtering 'acquired' in the dataset.features and seeing how many entries.
print(sum(dataset$status == "acquired", na.rm = TRUE))
#Number of 'acquired'=597 rows

#2- Find number of item in cluster

kmeanResults$size
kmeanResults2$size
kmeanResults3$size

#In the 2-means cluster, there are 2 clusters with respective number of items: 1 and 922.
#In the 3-means cluster, there are 3 clusters with respective number of items: 78, 844 and 1.
#In the 4-means cluster, there are 4 clusters with respective number of items: 15, 744, 163 and 1.

#3- Calculate the BCubed Precision

BCubedPrecision = NumberOfAcquiredItems/TotalNumberOfItemsInCluster

#3.1: Precision of acquired in 2-mean clusters after finding out how many acquired items are in each cluster
#Number of acquired/total number of items in the cluster

acquired2Clust1<-print(sum(dataset.features$status[kmeanResults$cluster =="1"]== "acquired", na.rm = TRUE))
print(1/1)

acquired2Clust2<-print(sum(dataset.features$status[kmeanResults$cluster =="2"]== "acquired", na.rm = TRUE))
print(596/922)

#3.1: Precision of acquired in 3-mean clusters after finding out how many acquired items are in each cluster

acquired3Clust1<-print(sum(dataset.features$status[kmeanResults2$cluster =="1"]== "acquired", na.rm = TRUE))
print(63/78)

acquired3Clust2<-print(sum(dataset.features$status[kmeanResults2$cluster =="2"]== "acquired", na.rm = TRUE))
print(533/844)

acquired3Clust3<-print(sum(dataset.features$status[kmeanResults2$cluster =="3"]== "acquired", na.rm = TRUE))
print(1/1)

#3.1: Precision of acquired in 4-mean clusters after finding out how many acquired items are in each cluster

acquired4Clust1<-print(sum(dataset.features$status[kmeanResults3$cluster =="1"]== "acquired", na.rm = TRUE))
print(10/15)

acquired4Clust2<-print(sum(dataset.features$status[kmeanResults3$cluster =="2"]== "acquired", na.rm = TRUE))
print(467/744)

acquired4Clust3<-print(sum(dataset.features$status[kmeanResults3$cluster =="3"]== "acquired", na.rm = TRUE))
print(119/163)

acquired4Clust4<-print(sum(dataset.features$status[kmeanResults3$cluster =="4"]== "acquired", na.rm = TRUE))
print(1/1)

#4- Calculate the BCubed Recall

BCubedRecall = NumberOfAcquiredItemsInCluster/TotalAcquiredItemsInDataset
#4.1: Recall of acquired in 2-mean clusters

Recall1Clust1= print(1/597) 
Recall1Clust2= print(596/597)

#4.2: Recall of acquired in 3-mean clusters 
  
Recall3Clust1= print(63/597) 
Recall3Clust2= print(533/597)
Recall3Clust3= print(1/597)

#4.3: Recall of acquired in 4-mean clusters 
  
Recall4Clust1= print(10/597) 
Recall4Clust2= print(467/597)
Recall4Clust3= print(119/597)
Recall4Clust4= print(1/597)
```